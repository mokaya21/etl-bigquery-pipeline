{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "253cbf1f-a77a-4596-9a04-8e715f4c0a2e",
   "metadata": {},
   "source": [
    "# ETL Pipeline: Excel + JSON → Transform → BigQuery  \n",
    "This notebook documents the full ETL workflow implemented as an Airflow DAG.\n",
    "\n",
    "\n",
    "\n",
    "## **Sections**\n",
    "1. Overview of the ETL pipeline  \n",
    "2. Import dependencies  \n",
    "3. DAG configuration\n",
    "4. Create BigQuery dataset\n",
    "5. Extraction steps  \n",
    "6. Transformation steps  \n",
    "7. Loading steps\n",
    "8. Data validation\n",
    "9. DAG Task Definitions\n",
    "10. DAG Orchestration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e131f0e-c90f-4de3-94c9-e8e65840dc71",
   "metadata": {},
   "source": [
    "## 1. ETL Pipeline Overview\n",
    "## Requirements:\n",
    "\n",
    "### i). Authentication\n",
    "Run the following command in your terminal to authenticate with Google Cloud:\n",
    "```bash\n",
    "gcloud auth application-default login\n",
    "```\n",
    "Ensure your Google account has these BigQuery permissions:\n",
    "- `bigquery.dataEditor`\n",
    "- `bigquery.jobUser`\n",
    "\n",
    "### ii). Python Packages\n",
    "Install required packages:\n",
    "```bash\n",
    "pip install pandas pyarrow google-cloud-bigquery google-cloud-bigquery-storage\n",
    "```\n",
    "\n",
    "**Troubleshooting:** If you encounter `pyarrow` or `protobuf` errors:\n",
    "- This environment uses `protobuf==4.25.8`\n",
    "- Install compatible versions:\n",
    "```bash\n",
    "pip install protobuf==4.25.8 pyarrow==22.0.0\n",
    "```\n",
    "\n",
    "### iii). Configuration\n",
    "- **Source files:** `/home/simon_mokaya/airflow/data`\n",
    "- **BigQuery project:** `etl-simon-001-479300`\n",
    "- **Dataset:** `staging_dataset`\n",
    "- **Dataset location:** `us-central1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597b427a-fbc6-48ca-9fa5-40b973b87510",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Diagrammatic Representation:\n",
    "The diagram below shows the full ETL workflow, including extraction, transformation, loading, and validation steps.\n",
    "![ETL Process Diagram](ETL_process.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec5ded5-0473-475f-9eb9-2d9b17d3e663",
   "metadata": {},
   "source": [
    "## 2. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b546d61c-74c2-4381-8931-3b7f4378b52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries and modules\n",
    "import pandas as pd\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from google.cloud import bigquery\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079812e1-7366-4164-bd19-8d62bab739c7",
   "metadata": {},
   "source": [
    "## 3. DAG Configuration\n",
    "### i). Define project IDs, dataset, file paths, and scheduling details for the DAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1de1cd-a2e1-4827-aa85-4e85e8c7339d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "PROJECT_ID = \"etl-simon-001-479300\"\n",
    "DATASET_ID = \"staging_dataset\"\n",
    "\n",
    "# File paths\n",
    "EXCEL_FILE_PATH = \"/home/simon_mokaya/airflow/data/Store_sales.xlsx\"\n",
    "JSON_FILE_PATH = \"/home/simon_mokaya/airflow/data/products.json\"\n",
    "\n",
    "# Default arguments\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': datetime(2024, 12, 3),\n",
    "    'retries': 3,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'email': ['mokayasimon495@gmail.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'email_on_success': False,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07bba63-e969-4703-8ec2-2d00dffe0104",
   "metadata": {},
   "source": [
    "### ii). DAG Instantiation\n",
    "\n",
    "Define schedule, description, and tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fd1b16-dada-4dc0-8058-4dd21dd6f991",
   "metadata": {},
   "outputs": [],
   "source": [
    "dag = DAG(\n",
    "    dag_id='etl_pipeline_workflow',\n",
    "    default_args=default_args,\n",
    "    description='Complete ETL pipeline: Extract, Transform, Load to BigQuery',\n",
    "    schedule='0 2 * * *',\n",
    "    catchup=False,\n",
    "    tags=['etl', 'bigquery', 'daily', 'staging'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e266b68-53de-4c95-aff6-6a99f4f31899",
   "metadata": {},
   "source": [
    "## 4. BigQuery Setup\n",
    "Create the BigQuery dataset if it does not exist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6045b17-518a-417b-b340-50ac5b7ca33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fxn to create bq dataset\n",
    "def create_bigquery_dataset():\n",
    "    \"\"\"Create BigQuery dataset if it doesn't exist\"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Creating/verifying BigQuery dataset: {DATASET_ID}\")\n",
    "\n",
    "        client = bigquery.Client(project=PROJECT_ID)\n",
    "        dataset_ref = f\"{PROJECT_ID}.{DATASET_ID}\"\n",
    "        dataset = bigquery.Dataset(dataset_ref)\n",
    "        dataset.location = \"us-central1\"\n",
    "\n",
    "        client.create_dataset(dataset, exists_ok=True)\n",
    "        logging.info(f\"Dataset '{DATASET_ID}' is ready\")\n",
    "\n",
    "        return \"Dataset created/verified successfully\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Dataset creation failed: {str(e)}\")\n",
    "        raise\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d7c602-bd18-44cf-8f21-932fc83630a2",
   "metadata": {},
   "source": [
    "## 5. Extraction Steps\n",
    "These functions read data from Excel and JSON files, save temporary CSV extracts, and pass the paths to downstream tasks using XCom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d084b37f-f862-4cea-b80c-7cc87025ef08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fxn to extract EXcel file\n",
    "def extract_excel_data(**context):\n",
    "    try:\n",
    "        df = pd.read_excel(EXCEL_FILE_PATH)\n",
    "        temp_path = '/tmp/extracted_excel.csv'\n",
    "        df.to_csv(temp_path, index=False)\n",
    "\n",
    "        context['ti'].xcom_push(key='excel_temp_path', value=temp_path)\n",
    "        context['ti'].xcom_push(key='excel_row_count', value=len(df))\n",
    "        return temp_path\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Excel extraction failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Fxn to extract JSON file\n",
    "def extract_json_data(**context):\n",
    "    try:\n",
    "        df = pd.read_json(JSON_FILE_PATH)\n",
    "        temp_path = '/tmp/extracted_json.csv'\n",
    "        df.to_csv(temp_path, index=False)\n",
    "\n",
    "        context['ti'].xcom_push(key='json_temp_path', value=temp_path)\n",
    "        context['ti'].xcom_push(key='json_row_count', value=len(df))\n",
    "        return temp_path\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"JSON extraction failed: {str(e)}\")\n",
    "        raise\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91399910-b6fd-4f67-aaf4-7eb316490519",
   "metadata": {},
   "source": [
    "## 6. Transformation Steps\n",
    "Transformation functions were created to clean, standardize, and validate the extracted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9c77c1-0fb9-46a5-a410-abb15e49d15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Excel file\n",
    "def transform_excel_data(**context):\n",
    "    try:\n",
    "        excel_path = context['ti'].xcom_pull(key='excel_temp_path', task_ids='extract_excel')\n",
    "        df = pd.read_csv(excel_path)\n",
    "        \n",
    "# Check if the first column contains comma-separated values \n",
    "        first_col = df.columns[0]\n",
    "        sample = df[first_col].dropna().astype(str).head(5)\n",
    "\n",
    "        if sample.str.contains(\",\").any():\n",
    "            # Single-column case -> split\n",
    "            df = df[first_col].str.split(\",\", expand=True)\n",
    "            df.columns = [\"date\", \"store_id\", \"product_id\", \"units_sold\", \"sales_amount\"]\n",
    "        else:\n",
    "            # Multi-column case -> assume Excel columns are already correct\n",
    "            expected_cols = [\"date\", \"store_id\", \"product_id\", \"units_sold\", \"sales_amount\"]\n",
    "            df = df.rename(columns={df.columns[i]: expected_cols[i] for i in range(len(expected_cols))})\n",
    "\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "        df[\"units_sold\"] = pd.to_numeric(df[\"units_sold\"], errors=\"coerce\").astype(\"int64\")\n",
    "        df[\"sales_amount\"] = pd.to_numeric(df[\"sales_amount\"], errors=\"coerce\").astype(\"float64\")\n",
    "\n",
    "        df = df.dropna()\n",
    "\n",
    "        transformed_path = '/tmp/transformed_excel.csv'\n",
    "        df.to_csv(transformed_path, index=False)\n",
    "\n",
    "        context['ti'].xcom_push(key='excel_transformed_path', value=transformed_path)\n",
    "        context['ti'].xcom_push(key='excel_final_count', value=len(df))\n",
    "\n",
    "        return transformed_path\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Excel transformation failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Transform JSON file\n",
    "def transform_json_data(**context):\n",
    "    try:\n",
    "        json_path = context['ti'].xcom_pull(key='json_temp_path', task_ids='extract_json')\n",
    "        df = pd.read_csv(json_path)\n",
    "\n",
    "        df[\"price\"] = pd.to_numeric(df[\"price\"], errors=\"coerce\").astype(\"float64\")\n",
    "        df = df.drop_duplicates().dropna()\n",
    "\n",
    "        transformed_path = '/tmp/transformed_json.csv'\n",
    "        df.to_csv(transformed_path, index=False)\n",
    "\n",
    "        context['ti'].xcom_push(key='json_transformed_path', value=transformed_path)\n",
    "        context['ti'].xcom_push(key='json_final_count', value=len(df))\n",
    "\n",
    "        return transformed_path\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"JSON transformation failed: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12d520f-16af-4e7b-91c5-9fb70e823634",
   "metadata": {},
   "source": [
    "## 7. Loading Steps\n",
    "Functions were created to load the preprocessed data into BigQuery staging tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1421df42-b5c2-4429-930c-e587f5196fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fxn to load a dataFrame to BigQuery staging table\n",
    "def load_staging_table(df, table_name):\n",
    "    table_id = f\"{PROJECT_ID}.{DATASET_ID}.{table_name}\"\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "        autodetect=True\n",
    "    )\n",
    "\n",
    "    client = bigquery.Client(project=PROJECT_ID)\n",
    "    job = client.load_table_from_dataframe(df, table_id, job_config=job_config)\n",
    "    job.result()\n",
    "\n",
    "    return table_id\n",
    "\n",
    "# Fxn to load transformed Excel file to BigQuery staging table\n",
    "def load_excel_to_bigquery(**context):\n",
    "    try:\n",
    "        transformed_path = context['ti'].xcom_pull(key='excel_transformed_path', task_ids='transform_excel')\n",
    "        df = pd.read_csv(transformed_path)\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "        table_id = load_staging_table(df, \"store_sales\")\n",
    "        return f\"Loaded {len(df)} rows → {table_id}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Excel load failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Fxn to load transformed Excel file to BigQuery staging table\n",
    "def load_json_to_bigquery(**context):\n",
    "    try:\n",
    "        transformed_path = context['ti'].xcom_pull(key='json_transformed_path', task_ids='transform_json')\n",
    "        df = pd.read_csv(transformed_path)\n",
    "\n",
    "        table_id = load_staging_table(df, \"products\")\n",
    "        return f\"Loaded {len(df)} rows → {table_id}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"JSON load failed: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a099be1a-a7d8-4204-becb-a00ca72bc430",
   "metadata": {},
   "source": [
    "## 8. Data Validation\n",
    "After loading the transformed data into BigQuery, a validation step is run to ensure data quality across all loaded tables. This task checks for:\n",
    "\n",
    "- Row counts – confirms that tables are not empty and have the expected number of rows.\n",
    "\n",
    "- Null values – scans key fields to ensure required columns are populated.\n",
    "\n",
    "- Duplicates – identifies duplicate records based on primary key or unique identifier fields.\n",
    "\n",
    "- Referential integrity – verifies that foreign keys match corresponding values in referenced tables (where applicable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f97dbc3-e24e-4e5e-9acd-5b2157a36551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fxn to run data validation\n",
    "def validate_data(**context):\n",
    "    \"\"\"\n",
    "    Run data quality checks on loaded BigQuery tables.\n",
    "    Validates: row counts, nulls, duplicates, and referential integrity.\n",
    "    \"\"\"\n",
    "    client = bigquery.Client(project=PROJECT_ID)\n",
    "    validation_results = []\n",
    "    has_critical_failure = False\n",
    "\n",
    "    try:\n",
    "        logging.info(\"=\"*60)\n",
    "        logging.info(\"Starting Data Quality Validation\")\n",
    "        logging.info(\"=\"*60)\n",
    "\n",
    "        # Get expected row counts from XCom\n",
    "        expected_sales_count = context['ti'].xcom_pull(key='excel_final_count', task_ids='transform_excel')\n",
    "        expected_products_count = context['ti'].xcom_pull(key='json_final_count', task_ids='transform_json')\n",
    "\n",
    "        # Row Count Checks\n",
    "        logging.info(\"Checking row counts...\")\n",
    "\n",
    "        sales_count = client.query(f\"\"\"\n",
    "            SELECT COUNT(*) as cnt\n",
    "            FROM `{PROJECT_ID}.{DATASET_ID}.store_sales`\n",
    "        \"\"\").result().to_dataframe().iloc[0]['cnt']\n",
    "\n",
    "        products_count = client.query(f\"\"\"\n",
    "            SELECT COUNT(*) as cnt\n",
    "            FROM `{PROJECT_ID}.{DATASET_ID}.products`\n",
    "        \"\"\").result().to_dataframe().iloc[0]['cnt']\n",
    "\n",
    "        # Check if tables are empty\n",
    "        if sales_count == 0:\n",
    "            has_critical_failure = True\n",
    "            validation_results.append(\"❌ CRITICAL: store_sales table is empty\")\n",
    "        else:\n",
    "            validation_results.append(f\"✅ store_sales has {sales_count} rows\")\n",
    "\n",
    "        if products_count == 0:\n",
    "            has_critical_failure = True\n",
    "            validation_results.append(\"❌ CRITICAL: products table is empty\")\n",
    "        else:\n",
    "            validation_results.append(f\"✅ products has {products_count} rows\")\n",
    "\n",
    "        # Verify row counts match expected from transformation\n",
    "        if sales_count != expected_sales_count:\n",
    "            has_critical_failure = True\n",
    "            validation_results.append(\n",
    "                f\"❌ CRITICAL: store_sales row count mismatch. Expected: {expected_sales_count}, Got: {sales_count}\"\n",
    "            )\n",
    "        else:\n",
    "            validation_results.append(f\"✅ store_sales row count matches transformed data\")\n",
    "\n",
    "        if products_count != expected_products_count:\n",
    "            has_critical_failure = True\n",
    "            validation_results.append(\n",
    "                f\"❌ CRITICAL: products row count mismatch. Expected: {expected_products_count}, Got: {products_count}\"\n",
    "            )\n",
    "        else:\n",
    "            validation_results.append(f\"✅ products row count matches transformed data\")\n",
    "\n",
    "        # Check for null values\n",
    "        logging.info(\"Checking for null values in critical columns...\")\n",
    "\n",
    "        null_check_sales = client.query(f\"\"\"\n",
    "            SELECT\n",
    "                COUNTIF(date IS NULL) as null_dates,\n",
    "                COUNTIF(product_id IS NULL) as null_product_ids,\n",
    "                COUNTIF(units_sold IS NULL) as null_units,\n",
    "                COUNTIF(sales_amount IS NULL) as null_amounts\n",
    "            FROM `{PROJECT_ID}.{DATASET_ID}.store_sales`\n",
    "        \"\"\").result().to_dataframe().iloc[0]\n",
    "\n",
    "        for col, null_count in null_check_sales.items():\n",
    "            col_name = col.replace('null_', '')\n",
    "            if null_count > 0:\n",
    "                has_critical_failure = True\n",
    "                validation_results.append(f\"❌ CRITICAL: {null_count} null values in store_sales.{col_name}\")\n",
    "            else:\n",
    "                validation_results.append(f\"✅ No nulls in store_sales.{col_name}\")\n",
    "\n",
    "        null_check_products = client.query(f\"\"\"\n",
    "            SELECT\n",
    "                COUNTIF(product_id IS NULL) as null_product_ids,\n",
    "                COUNTIF(product_name IS NULL) as null_names,\n",
    "                COUNTIF(price IS NULL) as null_prices\n",
    "            FROM `{PROJECT_ID}.{DATASET_ID}.products`\n",
    "        \"\"\").result().to_dataframe().iloc[0]\n",
    "\n",
    "        for col, null_count in null_check_products.items():\n",
    "            col_name = col.replace('null_', '')\n",
    "            if null_count > 0:\n",
    "                has_critical_failure = True\n",
    "                validation_results.append(f\"❌ CRITICAL: {null_count} null values in products.{col_name}\")\n",
    "            else:\n",
    "                validation_results.append(f\"✅ No nulls in products.{col_name}\")\n",
    "\n",
    "        # Check for duplicate records\n",
    "        logging.info(\"Checking for duplicate records...\")\n",
    "\n",
    "        duplicates_sales = client.query(f\"\"\"\n",
    "            SELECT product_id, date, COUNT(*) as dupes\n",
    "            FROM `{PROJECT_ID}.{DATASET_ID}.store_sales`\n",
    "            GROUP BY product_id, date\n",
    "            HAVING COUNT(*) > 1\n",
    "        \"\"\").result().to_dataframe()\n",
    "\n",
    "        if len(duplicates_sales) > 0:\n",
    "            has_critical_failure = True\n",
    "            validation_results.append(f\"❌ CRITICAL: {len(duplicates_sales)} duplicate records in store_sales\")\n",
    "            logging.warning(f\"Duplicate records:\\n{duplicates_sales.head()}\")\n",
    "        else:\n",
    "            validation_results.append(\"✅ No duplicates in store_sales\")\n",
    "\n",
    "        duplicates_products = client.query(f\"\"\"\n",
    "            SELECT product_id, COUNT(*) as dupes\n",
    "            FROM `{PROJECT_ID}.{DATASET_ID}.products`\n",
    "            GROUP BY product_id\n",
    "            HAVING COUNT(*) > 1\n",
    "        \"\"\").result().to_dataframe()\n",
    "\n",
    "        if len(duplicates_products) > 0:\n",
    "            has_critical_failure = True\n",
    "            validation_results.append(f\"❌ CRITICAL: {len(duplicates_products)} duplicate product_ids in products table\")\n",
    "            logging.warning(f\"Duplicate products:\\n{duplicates_products.head()}\")\n",
    "        else:\n",
    "            validation_results.append(\"✅ No duplicates in products\")\n",
    "\n",
    "        # Referential Integrity\n",
    "        logging.info(\"Checking referential integrity...\")\n",
    "\n",
    "        orphaned = client.query(f\"\"\"\n",
    "            SELECT COUNT(*) as orphaned_records\n",
    "            FROM `{PROJECT_ID}.{DATASET_ID}.store_sales` s\n",
    "            LEFT JOIN `{PROJECT_ID}.{DATASET_ID}.products` p\n",
    "            ON s.product_id = p.product_id\n",
    "            WHERE p.product_id IS NULL\n",
    "        \"\"\").result().to_dataframe().iloc[0]['orphaned_records']\n",
    "\n",
    "        if orphaned > 0:\n",
    "            has_critical_failure = True\n",
    "            validation_results.append(f\"❌ CRITICAL: {orphaned} sales records with invalid product_ids\")\n",
    "        else:\n",
    "            validation_results.append(\"✅ All sales records have valid product_ids\")\n",
    "\n",
    "        # Validating business logic\n",
    "        logging.info(\"Validating business rules...\")\n",
    "\n",
    "        ranges = client.query(f\"\"\"\n",
    "            SELECT\n",
    "                MIN(sales_amount) as min_amount,\n",
    "                MAX(sales_amount) as max_amount,\n",
    "                MIN(units_sold) as min_units,\n",
    "                MAX(units_sold) as max_units\n",
    "            FROM `{PROJECT_ID}.{DATASET_ID}.store_sales`\n",
    "        \"\"\").result().to_dataframe().iloc[0]\n",
    "\n",
    "       # Check for negative sales amounts\n",
    "        if ranges['min_amount'] < 0:\n",
    "            has_critical_failure = True\n",
    "            validation_results.append(f\"❌ CRITICAL: Negative sales_amount detected: {ranges['min_amount']}\")\n",
    "        else:\n",
    "            validation_results.append(f\"✅ sales_amount valid range: ${ranges['min_amount']:.2f} - ${ranges['max_amount']:.2f}\")\n",
    "\n",
    "        # Check for negative units\n",
    "        if ranges['min_units'] < 0:\n",
    "            has_critical_failure = True\n",
    "            validation_results.append(f\"❌ CRITICAL: Negative units_sold detected: {ranges['min_units']}\")\n",
    "        else:\n",
    "            validation_results.append(f\"✅ units_sold valid range: {ranges['min_units']} - {ranges['max_units']}\")\n",
    "\n",
    "        # Check product prices\n",
    "        price_ranges = client.query(f\"\"\"\n",
    "            SELECT\n",
    "                MIN(price) as min_price,\n",
    "                MAX(price) as max_price\n",
    "            FROM `{PROJECT_ID}.{DATASET_ID}.products`\n",
    "        \"\"\").result().to_dataframe().iloc[0]\n",
    "\n",
    "        if price_ranges['min_price'] <= 0:\n",
    "            has_critical_failure = True\n",
    "            validation_results.append(f\"❌ CRITICAL: Invalid product price detected: ${price_ranges['min_price']}\")\n",
    "        else:\n",
    "            validation_results.append(f\"✅ product prices valid range: ${price_ranges['min_price']:.2f} - ${price_ranges['max_price']:.2f}\")\n",
    "\n",
    "        # Generate report\n",
    "        report = \"\\n\".join(validation_results)\n",
    "        logging.info(\"\\n\" + \"=\"*60)\n",
    "        logging.info(\"DATA QUALITY REPORT\")\n",
    "        logging.info(\"=\"*60)\n",
    "        logging.info(report)\n",
    "        logging.info(\"=\"*60)\n",
    "\n",
    "        # Store results in XCom for potential email alerts\n",
    "        context['ti'].xcom_push(key='validation_report', value=report)\n",
    "        context['ti'].xcom_push(key='has_critical_failure', value=has_critical_failure)\n",
    "\n",
    "        # Fail the task if critical issues found\n",
    "        if has_critical_failure:\n",
    "            raise ValueError(f\"Data validation failed with critical errors:\\n{report}\")\n",
    "\n",
    "        logging.info(\"✅ All data quality checks passed!\")\n",
    "        return \"All validation checks passed successfully\"\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Validation failed: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cc463e-668f-4bd5-87d5-829d8f0e252c",
   "metadata": {},
   "source": [
    "## 9. DAG Task Definitions\n",
    "Create Airflow tasks using PythonOperators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec97150b-c606-4bd1-9f7d-9e349f7b8893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bq dataset creation\n",
    "create_dataset = PythonOperator(\n",
    "    task_id='create_bigquery_dataset',\n",
    "    python_callable=create_bigquery_dataset,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Extract .xlsx file\n",
    "extract_excel = PythonOperator(\n",
    "    task_id='extract_excel',\n",
    "    python_callable=extract_excel_data,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Extract .json file\n",
    "extract_json = PythonOperator(\n",
    "    task_id='extract_json',\n",
    "    python_callable=extract_json_data,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Transform .xlsx file\n",
    "transform_excel = PythonOperator(\n",
    "    task_id='transform_excel',\n",
    "    python_callable=transform_excel_data,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Transform .json file\n",
    "transform_json = PythonOperator(\n",
    "    task_id='transform_json',\n",
    "    python_callable=transform_json_data,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Load .xlsx file to bq\n",
    "load_excel = PythonOperator(\n",
    "    task_id='load_excel_to_bigquery',\n",
    "    python_callable=load_excel_to_bigquery,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Load .json file to bq\n",
    "load_json = PythonOperator(\n",
    "    task_id='load_json_to_bigquery',\n",
    "    python_callable=load_json_to_bigquery,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Validation task\n",
    "validate = PythonOperator(\n",
    "    task_id='validate_data',\n",
    "    python_callable=validate_data,\n",
    "    dag=dag,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23838da8-741b-4a3e-9d34-0d5306cc09d3",
   "metadata": {},
   "source": [
    "## 10. DAG Orchestration\n",
    "Define the sequence and parallel execution paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ce6d4c-256c-47e4-8171-c3fd67ba25d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset >> [extract_excel, extract_json]\n",
    "\n",
    "extract_excel >> transform_excel >> load_excel\n",
    "extract_json >> transform_json >> load_json\n",
    "\n",
    "[load_excel, load_json] >> validate\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
