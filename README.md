# ETL Pipeline: Excel + JSON ‚Üí Transform ‚Üí BigQuery

A production-ready ETL pipeline that extracts data from Excel and JSON sources, transforms it in Python, and loads it into Google BigQuery with comprehensive data quality validation.

##  Architecture

![ETL Process Diagram](ETL_process.png)

## üìã Features

- ‚úÖ **Automated Daily Extraction** - Reads from Excel and JSON sources
- ‚úÖ **Python-based Transformation** - Efficient data cleaning and standardization
- ‚úÖ **BigQuery Integration** - Loads cleaned data to Google Cloud
- ‚úÖ **Data Quality Validation** - Comprehensive checks for nulls, duplicates, and referential integrity
- ‚úÖ **Airflow Orchestration** - Automated scheduling with retry logic
- ‚úÖ **Email Alerts** - Notifications on pipeline failures

##  Quick Start

### Prerequisites

- Python 3.8+
- Apache Airflow 2.0+
- Google Cloud Project with BigQuery enabled
- Service account with BigQuery permissions

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/YOUR_USERNAME/etl-bigquery-pipeline.git
cd etl-bigquery-pipeline
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Set up Google Cloud authentication**
```bash
gcloud auth application-default login
```

4. **Configure the pipeline**

Edit `dags/etl_pipeline.py` and update:
```python
PROJECT_ID = "your-gcp-project-id"
DATASET_ID = "your_dataset_name"
EXCEL_FILE_PATH = "/path/to/your/Store_sales.xlsx"
JSON_FILE_PATH = "/path/to/your/products.json"
```

5. **Copy DAG to Airflow**
```bash
cp dags/etl_pipeline.py $AIRFLOW_HOME/dags/
```

6. **Start Airflow**
```bash
airflow webserver -p 8080
airflow scheduler
```

## üìä Pipeline Flow
```
Source Files ‚Üí Create Dataset ‚Üí Extract ‚Üí Transform ‚Üí Load ‚Üí Validate ‚Üí Success
                                                              ‚Üì
                                                         Email Alert (on failure)
```

### Pipeline Steps:

1. **Create BigQuery Dataset** - Ensures target dataset exists
2. **Extract Data** - Reads Excel (sales) and JSON (products) files
3. **Transform Data** - Cleans, validates, and standardizes data types
4. **Load to BigQuery** - Writes cleaned data to BigQuery tables
5. **Validate Data** - Runs quality checks:
   - Row count verification
   - Null value detection
   - Duplicate checking
   - Referential integrity
   - Business rule validation

##  Configuration

### Airflow DAG Configuration:
- **Schedule**: Daily at 2:00 AM UTC
- **Retries**: 3 attempts with 5-minute delays
- **Email Alerts**: On failure
- **Execution**: Sequential + Parallel (Excel and JSON paths run in parallel)

###  BigQuery Tables:
- `store_sales` - Sales transactions with date, product, units, and amounts
- `products` - Product catalog with IDs, names, and prices

## üìà Monitoring

Access the Airflow UI at `http://localhost:8080` to:
- Monitor DAG runs
- View task logs
- Check validation reports
- Manually trigger runs

## üîç Data Quality Checks

The pipeline validates:
- ‚úÖ No empty tables
- ‚úÖ Row counts match expected values
- ‚úÖ No null values in critical columns
- ‚úÖ No duplicate records
- ‚úÖ All foreign keys valid
- ‚úÖ No negative values in amounts/quantities
- ‚úÖ All prices are positive

## üõ†Ô∏è Technologies Used

- **Apache Airflow** - Workflow orchestration
- **Python/Pandas** - Data transformation
- **Google BigQuery** - Data warehouse
- **Google Cloud SDK** - Cloud authentication

## Contact

**Simon Mokaya**
- Email: mokayasimon495@gmail.com
- GitHub: https://github.com/mokaya21


