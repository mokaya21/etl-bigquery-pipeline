# ETL Pipeline: Excel + JSON â†’ Transform â†’ BigQuery

A production-ready ETL pipeline that extracts data from Excel and JSON sources, transforms it in Python, and loads it into Google BigQuery with comprehensive data quality validation.

##  Architecture

![ETL Process Diagram](ETL_process.png)

## ğŸ“‹ Features

- âœ… **Automated Daily Extraction** - Reads from Excel and JSON sources
- âœ… **Python-based Transformation** - Efficient data cleaning and standardization
- âœ… **BigQuery Integration** - Loads cleaned data to Google Cloud
- âœ… **Data Quality Validation** - Comprehensive checks for nulls, duplicates, and referential integrity
- âœ… **Airflow Orchestration** - Automated scheduling with retry logic
- âœ… **Email Alerts** - Notifications on pipeline failures

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- Apache Airflow 2.0+
- Google Cloud Project with BigQuery enabled
- Service account with BigQuery permissions

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/YOUR_USERNAME/etl-bigquery-pipeline.git
cd etl-bigquery-pipeline
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Set up Google Cloud authentication**
```bash
gcloud auth application-default login
```

4. **Configure the pipeline**

Edit `dags/etl_pipeline.py` and update:
```python
PROJECT_ID = "your-gcp-project-id"
DATASET_ID = "your_dataset_name"
EXCEL_FILE_PATH = "/path/to/your/Store_sales.xlsx"
JSON_FILE_PATH = "/path/to/your/products.json"
```

5. **Copy DAG to Airflow**
```bash
cp dags/etl_pipeline.py $AIRFLOW_HOME/dags/
```

6. **Start Airflow**
```bash
airflow webserver -p 8080
airflow scheduler
```

## ğŸ“Š Pipeline Flow
```
Source Files â†’ Create Dataset â†’ Extract â†’ Transform â†’ Load â†’ Validate â†’ Success
                                                              â†“
                                                         Email Alert (on failure)
```

### Pipeline Steps:

1. **Create BigQuery Dataset** - Ensures target dataset exists
2. **Extract Data** - Reads Excel (sales) and JSON (products) files
3. **Transform Data** - Cleans, validates, and standardizes data types
4. **Load to BigQuery** - Writes cleaned data to BigQuery tables
5. **Validate Data** - Runs quality checks:
   - Row count verification
   - Null value detection
   - Duplicate checking
   - Referential integrity
   - Business rule validation

##  Configuration

### Airflow DAG Configuration:
- **Schedule**: Daily at 2:00 AM UTC
- **Retries**: 3 attempts with 5-minute delays
- **Email Alerts**: On failure
- **Execution**: Sequential + Parallel (Excel and JSON paths run in parallel)

### BigQuery Tables:
- `store_sales` - Sales transactions with date, product, units, and amounts
- `products` - Product catalog with IDs, names, and prices

## ğŸ“ˆ Monitoring

Access the Airflow UI at `http://localhost:8080` to:
- Monitor DAG runs
- View task logs
- Check validation reports
- Manually trigger runs

## ğŸ” Data Quality Checks

The pipeline validates:
- âœ… No empty tables
- âœ… Row counts match expected values
- âœ… No null values in critical columns
- âœ… No duplicate records
- âœ… All foreign keys valid
- âœ… No negative values in amounts/quantities
- âœ… All prices are positive

## ğŸ› ï¸ Technologies Used

- **Apache Airflow** - Workflow orchestration
- **Python/Pandas** - Data transformation
- **Google BigQuery** - Data warehouse
- **Google Cloud SDK** - Cloud authentication

## ğŸ“§ Contact

**Simon Mokaya**
- Email: mokayasimon495@gmail.com
- GitHub: https://github.com/mokaya21


